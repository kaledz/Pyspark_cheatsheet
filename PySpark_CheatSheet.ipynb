{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9c9c03b",
   "metadata": {},
   "source": [
    "# PySpark Cheat Sheet\n",
    "A comprehensive reference of commonly used PySpark commands for data engineers and analysts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761f1a84",
   "metadata": {},
   "source": [
    "## Setup & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160aee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a0306e",
   "metadata": {},
   "source": [
    "## Read & Write Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3056a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"path/file.csv\", header=True, inferSchema=True)\n",
    "df = spark.read.json(\"path/file.json\")\n",
    "df = spark.read.parquet(\"path/file.parquet\")\n",
    "\n",
    "df.write.csv(\"output.csv\", header=True, mode=\"overwrite\")\n",
    "df.write.json(\"output.json\", mode=\"overwrite\")\n",
    "df.write.parquet(\"output.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4350b4e3",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bd128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()\n",
    "df.printSchema()\n",
    "df.columns\n",
    "df.describe().show()\n",
    "df.count()\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffc0eee",
   "metadata": {},
   "source": [
    "## DataFrame Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0ef32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, when, expr\n",
    "\n",
    "df.select(\"col1\", \"col2\")\n",
    "df.withColumn(\"new_col\", col(\"existing_col\") * 10)\n",
    "df.withColumnRenamed(\"old\", \"new\")\n",
    "df.drop(\"col_to_drop\")\n",
    "df.withColumn(\"flag\", when(col(\"value\") > 100, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4096893",
   "metadata": {},
   "source": [
    "## Filtering & Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ba825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(col(\"age\") > 30)\n",
    "df.where(\"age > 30 AND city = 'NY'\")\n",
    "df.orderBy(\"salary\")\n",
    "df.orderBy(col(\"salary\").desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5908104f",
   "metadata": {},
   "source": [
    "## Aggregations & Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fe6d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"department\").count()\n",
    "df.groupBy(\"department\").agg({\"salary\": \"avg\", \"age\": \"max\"})\n",
    "\n",
    "from pyspark.sql.functions import avg, max\n",
    "\n",
    "df.groupBy(\"department\").agg(\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    max(\"age\").alias(\"max_age\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b733a1",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4b212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join(df2, df1.id == df2.id, \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0522ac",
   "metadata": {},
   "source": [
    "## Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899e29d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank\n",
    "\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df.withColumn(\"rank\", rank().over(windowSpec))\n",
    "df.withColumn(\"row_num\", row_number().over(windowSpec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfb628d",
   "metadata": {},
   "source": [
    "## Type Casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aaa1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"age_int\", col(\"age\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfc51d0",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30105079",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()\n",
    "df.fillna({\"age\": 0, \"name\": \"Unknown\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ad18a",
   "metadata": {},
   "source": [
    "## User Defined Functions (UDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c6836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def upper_case(name):\n",
    "    return name.upper()\n",
    "\n",
    "upper_udf = udf(upper_case, StringType())\n",
    "df.withColumn(\"upper_name\", upper_udf(col(\"name\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d94ba6",
   "metadata": {},
   "source": [
    "## SQL with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f890721",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"people\")\n",
    "spark.sql(\"SELECT name, age FROM people WHERE age > 30\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3be571",
   "metadata": {},
   "source": [
    "## Schema Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c69ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.schema(schema).csv(\"people.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff736a76",
   "metadata": {},
   "source": [
    "## Partitioning & Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb14c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(4)\n",
    "df.coalesce(1)\n",
    "df.cache()\n",
    "df.persist()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}