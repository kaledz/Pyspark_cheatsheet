{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1725a657",
   "metadata": {},
   "source": [
    "# PySpark Cheat Sheet (Advanced)\n",
    "### Delta Lake on AWS | Apache Tika for Unstructured Data | JSON Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ee234b",
   "metadata": {},
   "source": [
    "## ðŸ§ª Delta Lake on AWS (Databricks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f20ad",
   "metadata": {},
   "source": [
    "### Setup Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4780c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing Delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"s3://your-bucket/path/to/delta-table\")\n",
    "\n",
    "# Reading Delta table\n",
    "df = spark.read.format(\"delta\").load(\"s3://your-bucket/path/to/delta-table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191d73e4",
   "metadata": {},
   "source": [
    "### Delta Table Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6bb596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Create DeltaTable object\n",
    "deltaTable = DeltaTable.forPath(spark, \"s3://your-bucket/path/to/delta-table\")\n",
    "\n",
    "# Update\n",
    "deltaTable.update(\n",
    "    condition = \"id = 5\",\n",
    "    set = { \"status\": \"'inactive'\" }\n",
    ")\n",
    "\n",
    "# Delete\n",
    "deltaTable.delete(\"status = 'inactive'\")\n",
    "\n",
    "# Merge (Upsert)\n",
    "deltaTable.alias(\"t\").merge(\n",
    "    source = updatesDF.alias(\"u\"),\n",
    "    condition = \"t.id = u.id\"\n",
    ").whenMatchedUpdate(set = { \"name\": \"u.name\" }) \\\n",
    " .whenNotMatchedInsert(values = { \"id\": \"u.id\", \"name\": \"u.name\" }) \\\n",
    " .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bd61f3",
   "metadata": {},
   "source": [
    "### Time Travel and Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f2e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read previous version\n",
    "df_v1 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"s3://your-bucket/path/to/delta-table\")\n",
    "\n",
    "# Read by timestamp\n",
    "df_time = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2024-01-01\").load(\"s3://your-bucket/path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef463d",
   "metadata": {},
   "source": [
    "### Optimize and Vacuum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1dafa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize for performance\n",
    "spark.sql(\"OPTIMIZE delta.`s3://your-bucket/path/to/delta-table`\")\n",
    "\n",
    "# Remove old data files\n",
    "spark.sql(\"VACUUM delta.`s3://your-bucket/path/to/delta-table` RETAIN 168 HOURS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b9db6f",
   "metadata": {},
   "source": [
    "## ðŸ“„ Parsing Unstructured Data with Apache Tika in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac4e05",
   "metadata": {},
   "source": [
    "### Setup Apache Tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a20923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# pip install tika\n",
    "\n",
    "from tika import parser\n",
    "\n",
    "# Parse document (PDF, Word, etc.)\n",
    "parsed = parser.from_file(\"/path/to/document.pdf\")\n",
    "text = parsed[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4e503a",
   "metadata": {},
   "source": [
    "### Convert Parsed Output to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf9ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Example with list of files\n",
    "files = [\"/docs/file1.pdf\", \"/docs/file2.docx\"]\n",
    "\n",
    "# Parse files into rows\n",
    "rows = [Row(filename=f, content=parser.from_file(f)[\"content\"]) for f in files]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_unstructured = spark.createDataFrame(rows)\n",
    "df_unstructured.show(truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0f4c80",
   "metadata": {},
   "source": [
    "### Clean and Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2976e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, lower, split\n",
    "\n",
    "df_cleaned = df_unstructured.withColumn(\"content\",\n",
    "    regexp_replace(lower(col(\"content\")), \"[^a-zA-Z\\s]\", \"\")\n",
    ").withColumn(\"words\", split(col(\"content\"), \"\\s+\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc9f13",
   "metadata": {},
   "source": [
    "## ðŸ§¾ JSON Parsing & Transformation in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a939b6c7",
   "metadata": {},
   "source": [
    "### Read JSON with Nested Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1bb46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = spark.read.json(\"s3://your-bucket/path/data.json\", multiLine=True)\n",
    "df_json.printSchema()\n",
    "df_json.show(truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fcabf6",
   "metadata": {},
   "source": [
    "### Extract and Flatten Nested Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = df_json.select(\n",
    "    col(\"user.id\").alias(\"user_id\"),\n",
    "    col(\"user.name\").alias(\"user_name\"),\n",
    "    col(\"event.type\").alias(\"event_type\"),\n",
    "    col(\"timestamp\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683de683",
   "metadata": {},
   "source": [
    "### Explode Arrays in JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e77d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df_exploded = df_json.withColumn(\"tag\", explode(col(\"tags\")))\n",
    "df_exploded.select(\"tag\", \"user.name\").show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}